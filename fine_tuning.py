# -*- coding: utf-8 -*-
"""Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QlBaU9qGDQiP7ETZOHU29d9oZn2Bmze1
"""

!nvidia-smi

"""### not required next 2 cell of code  ###"""

!pip install nbconvert
!jupyter nbconvert --to notebook --clear-output Fine_Tuning_LLM_with_LoRA.ipynb --output clean_notebook.ipynb
!pip install nbstripout
!nbstripout Fine-tuning.ipynb

!jupyter nbconvert --to notebook --clear-output Untitled1.ipynb --output clean_notebook.ipynb

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

"""### load tokernization ###

### laod model###
"""

!pip uninstall -y bitsandbytes
!pip install -U bitsandbytes==0.43.1

!pip uninstall -y bitsandbytes
!pip install -q transformers datasets accelerate peft trl
!pip install -q bitsandbytes==0.43.1

!nvidia-smi

!pip uninstall -y peft bitsandbytes
!pip install peft==0.9.0 transformers==4.36.2 accelerate datasets

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

from huggingface_hub import login
login()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

from datasets import Dataset

train_data = [
    {
        "text": "### Question: What is phishing?\n### Answer: Phishing is a cyber attack where attackers trick users into revealing sensitive information."
    },
    {
        "text": "### Question: What is ransomware?\n### Answer: Ransomware encrypts files and demands payment to restore access."
    },
    {
        "text": "### Question: What is malware?\n### Answer: Malware is malicious software designed to harm or exploit systems."
    },
    {
        "text": "### Question: What is a firewall?\n### Answer: A firewall monitors and controls incoming and outgoing network traffic."
    }
]

dataset = Dataset.from_list(train_data)
dataset

"""### cell no. 8  : tokernizer###"""

def tokenize_function(example):
    tokenized = tokenizer(
        example["text"],
        truncation=True,
        padding="max_length",
        max_length=256
    )
    tokenized["labels"] = tokenized["input_ids"].copy()
    return tokenized

tokenized_dataset = dataset.map(tokenize_function)

print(tokenized_dataset[0].keys())

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

!pip uninstall -y bitsandbytes

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "v_proj"]  # IMPORTANT
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

any("bnb" in str(type(m)) for m in model.modules())

"""### 10: Training Configuration ###"""

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./finetuned_llm",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    num_train_epochs=3,
    learning_rate=2e-4,
    fp16=True,
    logging_steps=1,
    save_steps=20,
    save_total_limit=2,
    report_to="none"
)

"""### 11: Train the Model ðŸš€###

"""

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.train()

import gradio as gr
import torch

def answer_question(user_question):
    prompt = f"### Question: {user_question}\n### Answer:"

    inputs = tokenizer(
        prompt,
        return_tensors="pt"
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=120,
            temperature=0.7,
            do_sample=True,
            top_p=0.9
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.split("### Answer:")[-1].strip()

interface = gr.Interface(
    fn=answer_question,
    inputs=gr.Textbox(
        lines=3,
        placeholder="Ask your question here...",
        label="Your Question"
    ),
    outputs=gr.Textbox(
        lines=5,
        label="Model Answer"
    ),
    title="Fine-Tuned LLM Q&A Bot",
    description="Ask a question and get an answer from your fine-tuned model."
)

interface.launch()

from google.colab import drive
drive.mount("/content/drive")